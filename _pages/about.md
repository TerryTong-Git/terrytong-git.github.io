---
layout: about
title: about
permalink: /
subtitle: <a href='https://cogcomp.seas.upenn.edu/'> Research Programmer @ Cognitive Computation Group, Upenn </a>. <a href='https://cs.ucdavis.edu'>4th year CS @ UC Davis </a>

profile:
  align: right
  image: profile.jpg
  image_circular: true # crops the image to make it circular
  more_info: >
    <p>Multimodal AI Lab,</p>
    <p>Amy Gutmann Hall,</p>
    <p>University of Pennsylvania, PA</p>

selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page

announcements:
  enabled: true # includes a list of news items
  scrollable: true # adds a vertical scroll bar if there are more than 3 news items
  limit: 5 # leave blank to include all the news in the `_news` folder

latest_posts:
  enabled: true
  scrollable: true # adds a vertical scroll bar if there are more than 3 new posts items
  limit: 3 # leave blank to include all the blog posts
---

I am a full-time **Research Programmer** at [Cognitive Computation Group](https://cogcomp.seas.upenn.edu/), [University of Pennsylvania](https://www.upenn.edu/) and fortunate to be advised by Professor [Dan Roth](https://www.cis.upenn.edu/~danroth/), and work closely with Professor [Surbhi Goel](https://www.surbhigoel.com/). I am also a  part-time 4th year undergrad in CS w/ a minor in Math at [UC Davis](https://www.ucdavis.edu), wrapping up my degree in my final quarter. Prior to Upenn, I was an undergraduate researcher at the Language Understanding and Knowledge Aquisition Lab ([LUKA](https://luka-group.github.io)), where I was fortunate to be co-advised by Professors [Muhao Chen](https://muhaochen.github.io) and [Zhe Zhao](https://sites.google.com/view/zhezhao). My work is supported by a [Provost Fellowship](https://urc.ucdavis.edu/PUF). I was also involved in the [Amazon Trustworthy AI Challenge](https://www.amazon.science/trusted-ai-challenge).

I currently work on __LLM Reasoning__ and neuro-symbolic methods to improve reliability, robustness, and performance in foundation models. Previously, I worked on post-training methods to reduce data poisoning vulnerabilities in language models. My broad interests are in computational and statistical methods to understand intelligence. I primarly focus on three research avenues: 

1) _Analysis of AI_ : Why do models make the decisions they do? How can we better evaluate models? 

2) _Solving AI_ : How can we improve reasoning and planning? What capabilities do foundation models unlock? How can we improve models beyond scaling up?

3) _Responsible AI_ : How can we reduce knowledge conflicts? How can we mitigate hallucinations?